{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batch_dataflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOK/YsRZQMsMxp/JgRplNXS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshalc108/Project-demo-pipelines/blob/main/batch_dataflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "asRH106Oy3YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-beam[gcp]"
      ],
      "metadata": {
        "id": "l3gfg7GfExgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZOnOyGp0EBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cac8f35-6e5d-407c-8b4a-7381c62e50da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery.py:2102: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
            "  self.table_reference.projectId = pcoll.pipeline.options.view_as(\n",
            "/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery_file_loads.py:1112: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
            "  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-02f0233b-0782-47b0-bed0-e820dd37991c.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'stage_location': 'gs://data_00/stage'}\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-02f0233b-0782-47b0-bed0-e820dd37991c.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'stage_location': 'gs://data_00/stage'}\n"
          ]
        }
      ],
      "source": [
        "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
        "import argparse\n",
        "import os\n",
        "from datetime import date\n",
        "from google.cloud import bigquery\n",
        "\n",
        "path1='bigquery-demo-335405-d7f1212692d0.json'\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=path1\n",
        "\n",
        "tab_id='batchdata.table1'\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--my-arg')\n",
        "args, beam_args = parser.parse_known_args()\n",
        "\n",
        "# Create and set your Pipeline Options.\n",
        "beam_options = PipelineOptions(\n",
        "    beam_args,\n",
        "    runner='DataflowRunner',\n",
        "    temp_location='gs://demo_data0/temp1999',\n",
        "    service_account_email='demo-python@bigquery-demo-335405.iam.gserviceaccount.com',\n",
        "    region='us-central1',\n",
        "    project='bigquery-demo-335405',\n",
        "    stage_location='gs://data_00/stage'\n",
        "    )\n",
        "\n",
        "#args = beam_options.view_as(MyOptions)\n",
        "#today= 2548566\n",
        "client = bigquery.Client()\n",
        "\n",
        "dataset_id = \"bigquery-demo-335405.batchdata\"\n",
        "\n",
        "try:\n",
        "  client.get_dataset(dataset_id)\n",
        "\n",
        "except:\n",
        "  dataset = bigquery.Dataset(dataset_id)  \n",
        "\n",
        "  dataset.location = \"US\"\n",
        "  dataset.description = \"dataset\"\n",
        "  dataset_ref = client.create_dataset(dataset)\n",
        "\n",
        "def to_json(fields):\n",
        "    json_str = {\"Region\":fields[0],\n",
        "                 \"Country\": fields[1],\n",
        "                 \"Item_Type\": fields[2],\n",
        "                 \"Sales_Channel\": fields[3],\n",
        "                \"Order_Priority\": fields[4],\n",
        "                \"Order_Date\": fields[5],\n",
        "                \"Order_ID\": fields[6],\n",
        "                \"Ship_Date\": fields[7],\n",
        "                \"Units_Sold\": fields[8],\n",
        "                \"Unit_Cost\": fields[9],\n",
        "                \"Total_Profit\": fields[10]\n",
        "\n",
        "                 \n",
        "                 \n",
        "                 }\n",
        "    return json_str\n",
        "schema_1='Region:STRING,Country:STRING,Item_Type:STRING,Sales_Channel:STRING,Order_Priority:STRING,Order_Date:STRING,Order_ID:INTEGER,Ship_Date:STRING,Units_Sold:INTEGER,Unit_Cost:FLOAT,Total_Profit:FLOAT '\n",
        "\n",
        "\n",
        "with beam.Pipeline(options=beam_options) as pipeline:\n",
        "    data_ingestion=(\n",
        "    pipeline\n",
        "    | beam.io.ReadFromText('gs://data_00/sales_record.csv',skip_header_lines=1)\n",
        "    | beam.Map(lambda x:x.split(','))\n",
        "    | \"filter Offline Sales_Channel\">> beam.Filter(lambda x:x[3]==\"Offline\")\n",
        "    | \"convert to json2\">> beam.Map(to_json)\n",
        "    | \"write to Offline table\">> beam.io.WriteToBigQuery(\n",
        "        tab_id,\n",
        "        schema=schema_1,\n",
        "        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
        "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
        "    )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zi6mWnGfU62T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ipCGq5GMuFkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}